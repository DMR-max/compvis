{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"execution":{"iopub.execute_input":"2025-04-01T16:07:03.175328Z","iopub.status.busy":"2025-04-01T16:07:03.175102Z","iopub.status.idle":"2025-04-01T16:07:04.119459Z","shell.execute_reply":"2025-04-01T16:07:04.118528Z","shell.execute_reply.started":"2025-04-01T16:07:03.175306Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/image-angle-pred-uhh-temp-yay-maybe/AngleOfPerson_20250330_175000.pkl\n","/kaggle/input/image-angle-pred-uhh-temp-yay-maybe/AngleOfPerson_20250331_042254.pkl\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["# Running TBD Pedestrian, image only angle prediction"]},{"cell_type":"code","execution_count":79,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T19:29:53.579999Z","iopub.status.busy":"2025-04-01T19:29:53.579756Z","iopub.status.idle":"2025-04-01T19:29:53.597356Z","shell.execute_reply":"2025-04-01T19:29:53.596657Z","shell.execute_reply.started":"2025-04-01T19:29:53.579970Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]}],"source":["import os\n","import pickle\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms, models\n","from PIL import Image\n","import numpy as np\n","from tqdm import tqdm\n","\n","###############################################################################\n","# 1. CONFIGURATION\n","###############################################################################\n","DATA_PKL     = \"/kaggle/input/image-angle-pred-uhh-temp-yay-maybe/AngleOfPerson_20250331_042254.pkl\"  # <-- Replace with your path\n","DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","BATCH_SIZE   = 100\n","EPOCHS       = 20\n","LEARNING_RATE = 0.004\n","TRAIN_SPLIT  = 0.8\n","# Basic image transforms\n","IMAGE_TRANSFORM = transforms.Compose([\n","    transforms.Resize((224, 224)), \n","    transforms.ToTensor(),\n","    # If using a pretrained ResNet, you typically want normalization:\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])\n","\n","print(f\"Using device: {DEVICE}\")"]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T19:29:53.598434Z","iopub.status.busy":"2025-04-01T19:29:53.598132Z","iopub.status.idle":"2025-04-01T19:29:53.611052Z","shell.execute_reply":"2025-04-01T19:29:53.610356Z","shell.execute_reply.started":"2025-04-01T19:29:53.598405Z"},"trusted":true},"outputs":[],"source":["###############################################################################\n","# 2. DATASET\n","###############################################################################\n","class ImageAngleDataset(Dataset):\n","    \"\"\"\n","    Dataset that returns:\n","      - image tensor\n","      - angle (float) as a label\n","    \"\"\"\n","    def __init__(self, data_list, transform=None):\n","        \"\"\"\n","        data_list: List of (image_path, angle)\n","        transform: TorchVision transforms for images\n","        \"\"\"\n","        self.data_list = []\n","        self.transform = transform\n","        print(\"Creating dataset!\")\n","        for item_a, item_b in tqdm(data_list):\n","            modified_item_a = Image.fromarray(item_a)\n","            if self.transform:\n","                modified_item_a = self.transform(modified_item_a)#.to(DEVICE)\n","            \n","            modified_item_b = torch.tensor(item_b, dtype=torch.float32).unsqueeze(0)#.to(DEVICE)\n","            self.data_list.append((modified_item_a, modified_item_b))\n","\n","    def __len__(self):\n","        return len(self.data_list)\n","\n","    def __getitem__(self, idx):\n","        return self.data_list[idx]\n","        # Load image\n","        image = Image.fromarray(image_path)\n","        # image = torch.from_numpy(np.transpose(image, (2,0,1))).float()\n","        if self.transform:\n","            image = self.transform(image)\n","        # Convert angle to float tensor [1,]\n","        angle_tensor = torch.tensor(angle, dtype=torch.float32).unsqueeze(0)\n","        return image, angle_tensor"]},{"cell_type":"code","execution_count":81,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T19:29:53.612959Z","iopub.status.busy":"2025-04-01T19:29:53.612746Z","iopub.status.idle":"2025-04-01T19:29:53.630773Z","shell.execute_reply":"2025-04-01T19:29:53.629937Z","shell.execute_reply.started":"2025-04-01T19:29:53.612940Z"},"trusted":true},"outputs":[],"source":["###############################################################################\n","# 3. MODEL: Simple ResNet-based regressor\n","###############################################################################\n","class ImageRegressor(nn.Module):\n","    def __init__(self, pretrained=True):\n","        \"\"\"\n","        If pretrained=True, uses pretrained ImageNet weights.\n","        If pretrained=False, initializes from scratch.\n","        \"\"\"\n","        super().__init__()\n","        # Use a ResNet18 as the backbone\n","        if pretrained:\n","            backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n","        else:\n","            backbone = models.resnet18(weights=None)\n","\n","        # Remove the final classification layer\n","        num_feats = backbone.fc.in_features\n","        backbone.fc = nn.Identity()\n","\n","        self.backbone = backbone\n","        # Final linear to produce 1 output (angle)\n","        #       But with an extra layer in between to smoothen the process\n","        self.fc_before = nn.Linear(num_feats, 32)\n","        self.fc = nn.Linear(32, 1)\n","\n","    def forward(self, x):\n","        # x: [batch_size, 3, H, W]\n","        features = self.backbone(x)   # [batch_size, 512] for ResNet18\n","        before_out = self.fc_before(features)\n","        out = self.fc(before_out)       # [batch_size, 1]\n","        return out"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T19:29:53.632087Z","iopub.status.busy":"2025-04-01T19:29:53.631717Z","iopub.status.idle":"2025-04-01T19:29:53.646417Z","shell.execute_reply":"2025-04-01T19:29:53.645602Z","shell.execute_reply.started":"2025-04-01T19:29:53.632024Z"},"trusted":true},"outputs":[],"source":["###############################################################################\n","# 4. TRAINING & TESTING\n","###############################################################################\n","def circular_error(pred, actual):\n","    \"\"\"\n","    Computes the circular error (in degrees) between predicted and actual angles.\n","    The error is defined as the minimum of the absolute difference and 360 minus that difference.\n","    \"\"\"\n","    diff = abs(pred - actual)\n","    return diff if diff <= 180 else 360 - diff\n","\n","def train_model(model, train_loader, epochs=10, lr=1e-3):\n","    model.to(DEVICE)\n","    criterion = nn.MSELoss().to(DEVICE)\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    \n","\n","    for epoch in range(epochs):\n","        checkpoint_path = f\"checkpoint_angle_pred_images_TBD_epoch_{epoch+1}.pth\"\n","        model.train()\n","        total_loss = 0.0\n","\n","        for images, angles in tqdm(train_loader):\n","            images = images.to(DEVICE)\n","            angles = angles.to(DEVICE)\n","\n","            optimizer.zero_grad()\n","            preds = model(images)\n","            loss = criterion(preds, angles)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        print(f\"Epoch [{epoch+1}/{epochs}]  Train Loss: {avg_loss:.4f}\")\n","        if (epoch+1) % 5 == 0:\n","            torch.save(model.state_dict(), checkpoint_path)\n","\n","def test_model(model, test_loader):\n","    model.to(DEVICE)\n","    model.eval()\n","    criterion = nn.MSELoss()\n","    total_loss = 0.0\n","\n","    # Optional: store predictions for further analysis\n","    all_preds = []\n","    all_labels = []\n","    total_circular_error = 0.0\n","    count = 0\n","\n","    with torch.no_grad():\n","        for images, angles in tqdm(test_loader):\n","            images = images.to(DEVICE)\n","            angles = angles.to(DEVICE)\n","\n","            preds = model(images)\n","            loss = criterion(preds, angles)\n","            total_loss += loss.item()\n","\n","            preds_list = preds.cpu().view(-1).tolist()\n","            angles_list = angles.cpu().view(-1).tolist()\n","            all_preds.extend(preds_list)\n","            all_labels.extend(angles_list)\n","\n","            for p, a in zip(preds_list, angles_list):\n","                err = circular_error(p, a)\n","                total_circular_error += err\n","                count += 1\n","\n","    avg_loss = total_loss / len(test_loader)\n","    avg_circular_error = total_circular_error / count if count > 0 else 0.0\n","\n","    print(f\"Test Loss: {avg_loss:.4f}\")\n","    print(\"Sample Predictions vs Actual with Circular Error:\")\n","    # for i in range(len(all_preds)):\n","    #     err = circular_error(all_preds[i], all_labels[i])\n","    #     print(f\"  Pred: {all_preds[i]:.2f}, Actual: {all_labels[i]:.2f}, Circular Error: {err:.2f}\")\n","    print(f\"Average Circular Error: {avg_circular_error:.2f}\")\n","    return all_preds, all_labels"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T19:29:53.647521Z","iopub.status.busy":"2025-04-01T19:29:53.647205Z","iopub.status.idle":"2025-04-01T19:49:27.076087Z","shell.execute_reply":"2025-04-01T19:49:27.075131Z","shell.execute_reply.started":"2025-04-01T19:29:53.647487Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded 35442 samples from /kaggle/input/image-angle-pred-uhh-temp-yay-maybe/AngleOfPerson_20250331_042254.pkl\n","Creating dataset!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 35442/35442 [00:56<00:00, 628.87it/s]\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 175MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["ImageRegressor(\n","  (backbone): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Identity()\n","  )\n","  (fc_before): Linear(in_features=512, out_features=32, bias=True)\n","  (fc): Linear(in_features=32, out_features=1, bias=True)\n",")\n","Starting Training ...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/20]  Train Loss: 4074.6598\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/20]  Train Loss: 2289.6262\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/20]  Train Loss: 1824.7184\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/20]  Train Loss: 1590.4435\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/20]  Train Loss: 1457.1083\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [6/20]  Train Loss: 1179.4808\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [7/20]  Train Loss: 929.6615\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [8/20]  Train Loss: 770.8657\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [9/20]  Train Loss: 645.2681\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [10/20]  Train Loss: 576.0873\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [11/20]  Train Loss: 556.2588\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [12/20]  Train Loss: 377.6495\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [13/20]  Train Loss: 295.6406\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [14/20]  Train Loss: 238.3053\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [15/20]  Train Loss: 269.3950\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [16/20]  Train Loss: 318.7275\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [17/20]  Train Loss: 344.0248\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [18/20]  Train Loss: 249.7903\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [19/20]  Train Loss: 143.3048\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 284/284 [00:55<00:00,  5.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [20/20]  Train Loss: 100.9824\n","Starting Testing ...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 71/71 [00:06<00:00, 11.23it/s]"]},{"name":"stdout","output_type":"stream","text":["Test Loss: 878.7969\n","Sample Predictions vs Actual with Circular Error:\n","Average Circular Error: 12.95\n","Done!\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# 1) Load data_list from .pkl\n","with open(DATA_PKL, \"rb\") as f:\n","    data_list = pickle.load(f)\n","print(f\"Loaded {len(data_list)} samples from {DATA_PKL}\")\n","\n","# 2) Create dataset\n","dataset = ImageAngleDataset(data_list, transform=IMAGE_TRANSFORM)\n","\n","# 3) Split into train/test\n","train_size = int(TRAIN_SPLIT * len(dataset))\n","test_size = len(dataset) - train_size\n","train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n","\n","# 4) Initialize model\n","model = ImageRegressor(pretrained=True)\n","print(model)\n","\n","# 5) Train\n","print(\"Starting Training ...\")\n","train_model(model, train_loader, epochs=EPOCHS, lr=LEARNING_RATE)\n","\n","# 6) Test\n","print(\"Starting Testing ...\")\n","test_model(model, test_loader)\n","\n","print(\"Done!\")"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T19:49:32.022257Z","iopub.status.busy":"2025-04-01T19:49:32.021906Z","iopub.status.idle":"2025-04-01T19:49:38.456953Z","shell.execute_reply":"2025-04-01T19:49:38.455964Z","shell.execute_reply.started":"2025-04-01T19:49:32.022226Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading checkpoint...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 71/71 [00:06<00:00, 11.17it/s]"]},{"name":"stdout","output_type":"stream","text":["Test Loss: 878.7969\n","Sample Predictions vs Actual with Circular Error:\n","Average Circular Error: 12.95\n","ACE result is\n","13.741358609401065\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["checkpoint_path = \"/kaggle/working/checkpoint_angle_pred_images_TBD_epoch_20.pth\"\n","\n","if os.path.isfile(checkpoint_path):\n","    print(\"Loading checkpoint...\")\n","    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=True)\n","    model.load_state_dict(checkpoint)\n","\n","    all_preds, all_labels = test_model(model, test_loader)\n","    print(\"ACE result is\")\n","    all_preds = np.array(all_preds) % 360\n","    all_labels = np.array(all_labels) % 360\n","    ACE = np.sum(abs(all_preds - all_labels)) / len(all_preds)\n","    print(ACE)"]},{"cell_type":"markdown","metadata":{},"source":["# Creating keypoints annotations"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T17:44:26.966074Z","iopub.status.busy":"2025-04-01T17:44:26.965851Z","iopub.status.idle":"2025-04-01T17:44:32.179784Z","shell.execute_reply":"2025-04-01T17:44:32.178649Z","shell.execute_reply.started":"2025-04-01T17:44:26.966053Z"},"trusted":true},"outputs":[],"source":["import os\n","from scipy.io import loadmat\n","import cv2\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset\n","from tqdm import tqdm\n","import pickle\n","import datetime\n","from PIL import Image"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T17:44:32.182871Z","iopub.status.busy":"2025-04-01T17:44:32.182328Z","iopub.status.idle":"2025-04-01T17:44:45.505844Z","shell.execute_reply":"2025-04-01T17:44:45.505114Z","shell.execute_reply.started":"2025-04-01T17:44:32.182835Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/input/image-angle-pred-uhh-temp-yay-maybe/AngleOfPerson_20250331_042254.pkl', 'rb') as f:\n","    loaded_data = pickle.load(f)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T17:44:45.507469Z","iopub.status.busy":"2025-04-01T17:44:45.507137Z","iopub.status.idle":"2025-04-01T17:44:57.037476Z","shell.execute_reply":"2025-04-01T17:44:57.036615Z","shell.execute_reply.started":"2025-04-01T17:44:45.507440Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["### Stores the obtained images as a zip file in the given path\n","def obtain_all_pictures(full_data, path, name):\n","    temp_path = os.path.join(path, 'temp_image/')\n","    if not os.path.exists(temp_path):\n","        os.makedirs(temp_path)\n","\n","    current_item = 0\n","    for i in range(len(full_data)):\n","        image = full_data[i][0]\n","        image = Image.fromarray(image.astype('uint8')).convert('RGB')\n","        image.save(os.path.join(temp_path, 'image_' + str(i) + '.jpg'))\n","\n","    # import shutil\n","    # shutil.make_archive(os.path.join(path, name), 'zip', temp_path)\n","    # shutil.rmtree(temp_path)\n","\n","obtain_all_pictures(loaded_data, \"/kaggle/working/annotation_data/images/\", 'images')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T17:44:57.038530Z","iopub.status.busy":"2025-04-01T17:44:57.038303Z","iopub.status.idle":"2025-04-01T17:45:00.294481Z","shell.execute_reply":"2025-04-01T17:45:00.293544Z","shell.execute_reply.started":"2025-04-01T17:44:57.038512Z"},"trusted":true},"outputs":[],"source":["with open('/kaggle/working/annotation_data/AngleOfPerson_20250331_042254.pkl', 'wb') as f:\n","    pickle.dump(loaded_data, f)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T17:45:00.295825Z","iopub.status.busy":"2025-04-01T17:45:00.295485Z","iopub.status.idle":"2025-04-01T17:45:00.302416Z","shell.execute_reply":"2025-04-01T17:45:00.301609Z","shell.execute_reply.started":"2025-04-01T17:45:00.295793Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['AngleOfPerson_20250331_042254.pkl', 'images']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir(\"/kaggle/working/annotation_data\")"]},{"cell_type":"code","execution_count":11,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2025-04-01T18:05:07.627872Z","iopub.status.busy":"2025-04-01T18:05:07.627544Z","iopub.status.idle":"2025-04-01T18:15:06.147799Z","shell.execute_reply":"2025-04-01T18:15:06.146506Z","shell.execute_reply.started":"2025-04-01T18:05:07.627848Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.99)\n","Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n","Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\n","Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\n","Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\n","Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\n","Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\n","Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n","Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\n","Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\n","Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n","Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n","35442\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 35442/35442 [09:54<00:00, 59.58it/s]"]},{"name":"stdout","output_type":"stream","text":["35442\n"]},{"name":"stderr","output_type":"stream","text":["\n"]},{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'kaggle/working/annotation_data/'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-9a0adeda7d32>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Save the keypoints data to a pickle file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kaggle/working/annotation_data/'"]}],"source":["!pip install ultralytics\n","from ultralytics import YOLO\n","import os\n","import pickle\n","from tqdm import tqdm\n","\n","# Load a COCO-pretrained YOLO12n model\n","model = YOLO(\"yolo11m-pose.pt\")  # load an official model\n","\n","def single_run(path_input, path_output):\n","    results = model(path_input, save=True, show=True, show_conf=False, show_labels=False, max_det = 1, verbose=False)  # predict on an image\n","\n","    for result in results:\n","        keypoints = result.keypoints  # Keypoints object for pose outputs\n","\n","    xyn = keypoints.xyn \n","\n","    f = open(path_output, \"w\")\n","    for i in range(len(xyn)):\n","        f.write(str(xyn[i]) + \"\\n\")\n","    f.close()\n","\n","\n","path_input = \"/kaggle/working/annotation_data/images/temp_image/\"\n","path_output = \"/kaggle/working/annotation_data/KeypointsData.pkl\"\n","images_dir = path_input\n","output_pickle = path_output\n","keypoints_data = {}\n","print(len(os.listdir(images_dir)))\n","# Process all images in the directory\n","for filename in tqdm(os.listdir(images_dir)):\n","    if filename.endswith(\".jpg\"):\n","        results = model(\n","            os.path.join(images_dir, filename),\n","            save=False,\n","            show_conf=False,\n","            show_labels=False,\n","            verbose=False,\n","            max_det=1,\n","            device=DEVICE\n","        )  # predict on an image\n","\n","        for result in results:\n","            keypoints = result.keypoints  # Keypoints object for pose outputs\n","            xyn = keypoints.xyn.tolist()  # Convert normalized coordinates to a list\n","            keypoints_data[filename] = xyn  # Save keypoints for the image\n","print(len(keypoints_data))\n","\n","# Save the keypoints data to a pickle file\n","with open(output_pickle, \"wb\") as f:\n","    pickle.dump(keypoints_data, f)\n","\n","print(f\"Keypoints data saved to {output_pickle}\")\n","\n","# total_run(\"/kaggle/working/annotation_data/images/temp_image/\", \"kaggle/working/annotation_data\")"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T18:19:02.964047Z","iopub.status.busy":"2025-04-01T18:19:02.963741Z","iopub.status.idle":"2025-04-01T18:19:02.968950Z","shell.execute_reply":"2025-04-01T18:19:02.968277Z","shell.execute_reply.started":"2025-04-01T18:19:02.964026Z"},"trusted":true},"outputs":[{"data":{"text/plain":["35442"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["len(keypoints_data)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T18:17:00.560523Z","iopub.status.busy":"2025-04-01T18:17:00.560178Z","iopub.status.idle":"2025-04-01T18:17:00.566124Z","shell.execute_reply":"2025-04-01T18:17:00.565400Z","shell.execute_reply.started":"2025-04-01T18:17:00.560495Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['KeypointsData.pkl', 'AngleOfPerson_20250331_042254.pkl', 'images']"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["os.listdir(\"/kaggle/working/annotation_data\")"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T18:21:58.745775Z","iopub.status.busy":"2025-04-01T18:21:58.745488Z","iopub.status.idle":"2025-04-01T18:21:58.770119Z","shell.execute_reply":"2025-04-01T18:21:58.769450Z","shell.execute_reply.started":"2025-04-01T18:21:58.745753Z"},"trusted":true},"outputs":[{"data":{"text/plain":["35442"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["len(os.listdir(\"/kaggle/working/annotation_data/images/temp_image\"))"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T18:22:16.733358Z","iopub.status.busy":"2025-04-01T18:22:16.733061Z","iopub.status.idle":"2025-04-01T18:22:18.463595Z","shell.execute_reply":"2025-04-01T18:22:18.462634Z","shell.execute_reply.started":"2025-04-01T18:22:16.733337Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["35442\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 35442/35442 [00:00<00:00, 582869.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Keypoints data saved to /kaggle/working/annotation_data/CombinedData.pkl\n"]}],"source":["path_dir = \"/kaggle/working/annotation_data/\"\n","output_pickle = \"/kaggle/working/annotation_data/CombinedData.pkl\"\n","img_dir = \"/kaggle/working/annotation_data/images/temp_image/\"\n","directory = path_dir\n","result = []\n","\n","with open(directory + 'AngleOfPerson_20250331_042254.pkl', 'rb') as f:\n","    loaded_data = pickle.load(f)\n","with open(directory + 'KeypointsData.pkl', 'rb') as f:\n","    loaded_keypoints = pickle.load(f)\n","\n","# Iterate through the images (image_0.jpg to image_432.jpg)\n","print(len(loaded_keypoints))\n","for i in tqdm(range(len(loaded_keypoints))):\n","    image_key = f\"image_{i}.jpg\"\n","    if image_key in loaded_keypoints:\n","        keypoints_value = loaded_keypoints[image_key]\n","        angle_value = loaded_data[i][1]  # Get the second value from loaded_data[i]\n","        result.append((keypoints_value, angle_value))  # Create the tuple and add to the list\n","\n","# output_pickle = \"C:/Users/Bulut/Documents/GitHub/Skelet/CombinedData.pkl\"\n","with open(output_pickle, \"wb\") as f:\n","    pickle.dump(result, f)\n","\n","print(f\"Keypoints data saved to {output_pickle}\")"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T18:54:56.977646Z","iopub.status.busy":"2025-04-01T18:54:56.977272Z","iopub.status.idle":"2025-04-01T18:55:03.296973Z","shell.execute_reply":"2025-04-01T18:55:03.296217Z","shell.execute_reply.started":"2025-04-01T18:54:56.977619Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["35442\n","34657\n","Filtered data saved to: /kaggle/working/annotation_data/FinalKeypointSet35K.pkl\n"]}],"source":["# Adjust paths as needed\n","pkl_path = \"/kaggle/working/annotation_data/CombinedData.pkl\"\n","images_dir = \"/kaggle/working/annotation_data/images/temp_image/\"\n","output_pkl_path = \"/kaggle/working/annotation_data/FinalKeypointSet35K.pkl\"\n","\n","# Load your CombinedData.pkl\n","with open(pkl_path, 'rb') as f:\n","    data_dict = pickle.load(f)  # Suppose it's a list of [keypoints, angle]\n","    \n","#print(f\"Loaded {len(data_dict)} samples from {pkl_path}\")\n","#print(f\"Example data: {data_dict[0][0]}\")\n","#print(f\"Example data: {data_dict[0][1]}\")\n","\n","\n","filtered_data_list = []\n","print(len(data_dict))\n","\n","for i, item in enumerate(data_dict):\n","    keypoints = item[0]\n","    angle = torch.tensor(item[1], dtype=torch.float32).unsqueeze(0)   # Labels are the angle (single value)\n","\n","    # Convert to torch tensors for checking shape (or you can just check Python lists)\n","    sequence = torch.tensor(keypoints, dtype=torch.float32)\n","    if sequence.ndimension() == 3 and sequence.shape[0] == 1:\n","        sequence = sequence.squeeze(0)  # Remove the first singleton dimension\n","\n","    # Check if sequence is empty or has invalid shape\n","    # Example: we want sequence of shape (seq_len, 2) and seq_len>0\n","    if sequence.shape[0] == 0 or sequence.shape[1] != 2:\n","        # Skip this row\n","        continue\n","    \n","    # If we reach here, the row is valid\n","    # The corresponding image is: \"image_{i}.jpg\" or some pattern\n","    image_path = os.path.join(images_dir, f\"image_{i}.jpg\")\n","\n","    # (Optionally) check if the image actually exists on disk\n","    if not os.path.isfile(image_path):\n","        print(f\"Warning: Image not found at {image_path}, skipping.\")\n","        continue\n","\n","    # Keep the data\n","    filtered_data_list.append((image_path, sequence, angle))\n","\n","print(len(filtered_data_list))\n","\n","#If desired, save the filtered data to a new pkl\n","with open(output_pkl_path, 'wb') as f:\n","    pickle.dump(filtered_data_list, f)\n","\n","\n","print(f\"Filtered data saved to: {output_pkl_path}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Running TBD Pedestrian, image + keypoints angle prediction"]},{"cell_type":"code","execution_count":74,"metadata":{"execution":{"iopub.execute_input":"2025-04-01T19:28:30.024267Z","iopub.status.busy":"2025-04-01T19:28:30.023821Z","iopub.status.idle":"2025-04-01T19:29:53.505645Z","shell.execute_reply":"2025-04-01T19:29:53.504818Z","shell.execute_reply.started":"2025-04-01T19:28:30.024226Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-74-1fbbf4dc68de>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  sequence = torch.tensor(item[1], dtype=torch.float32)  # Ensure this is a tensor of shape (seq_len, 2)\n","<ipython-input-74-1fbbf4dc68de>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  label = torch.tensor(item[2], dtype=torch.float32).unsqueeze(0)   # Labels are the angle (single value)\n","<ipython-input-74-1fbbf4dc68de>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  self.data = [torch.tensor(seq, dtype=torch.float32).to(device) for seq in data]  # Convert to tensor\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20, Loss: 27926.6699\n","Epoch 2/20, Loss: 16776.3227\n","Epoch 3/20, Loss: 10011.9702\n","Epoch 4/20, Loss: 8735.7185\n","Epoch 5/20, Loss: 8109.4794\n","Epoch 6/20, Loss: 7349.8417\n","Epoch 7/20, Loss: 7202.8502\n","Epoch 8/20, Loss: 7093.6007\n","Epoch 9/20, Loss: 6958.2085\n","Epoch 10/20, Loss: 6791.2280\n","Epoch 11/20, Loss: 6664.6166\n","Epoch 12/20, Loss: 6578.2953\n","Epoch 13/20, Loss: 6457.5673\n","Epoch 14/20, Loss: 6380.6527\n","Epoch 15/20, Loss: 6350.3630\n","Epoch 16/20, Loss: 6253.4224\n","Epoch 17/20, Loss: 6216.8052\n","Epoch 18/20, Loss: 6126.6757\n","Epoch 19/20, Loss: 6114.6113\n","Epoch 20/20, Loss: 6046.6032\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 70/70 [00:00<00:00, 268.84it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Test Loss: 5913.8797\n","Average Circular Error: 57.07\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import pickle\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.nn.utils.rnn import pad_sequence\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load dataset from .pkl file\n","directory = \"/kaggle/working/annotation_data/\"  # Update this to the correct path\n","with open(directory + 'FinalKeypointSet35K.pkl', 'rb') as f:\n","    data_dict = pickle.load(f)  # Assuming it's stored as a dictionary\n","\n","# Flatten data_dict to ensure each data sample is a 2D tensor of shape (seq_len, 2)\n","data = []\n","labels = []\n","for item in data_dict:\n","    # Convert each sequence into a tensor of shape (seq_len, 2) and each label as a float tensor\n","    sequence = torch.tensor(item[1], dtype=torch.float32)  # Ensure this is a tensor of shape (seq_len, 2)\n","    label = torch.tensor(item[2], dtype=torch.float32).unsqueeze(0)   # Labels are the angle (single value)\n","    data.append(sequence)\n","    labels.append(label)\n","\n","# Padding function for variable-length sequences\n","def pad_batch(batch):\n","    sequences, labels = zip(*batch)\n","    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)  # Pad with zeros\n","    return padded_sequences, torch.stack(labels)\n","\n","# Dataset Class\n","class CoordDataset(Dataset):\n","    def __init__(self, data, labels):\n","        self.data = [torch.tensor(seq, dtype=torch.float32).to(device) for seq in data]  # Convert to tensor\n","        self.labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(device)  # Convert to tensor and reshape\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        x = self.data[idx]  # Tensor of shape (seq_len, 2)\n","        y = self.labels[idx]  # Keep as a single value (not sin/cos)\n","        return x, y\n","\n","# Split dataset into training and testing (90% training, 10% testing)\n","dataset = CoordDataset(data, labels)\n","train_size = int(0.8 * len(dataset))\n","test_size = len(dataset) - train_size\n","train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","# Create DataLoaders for training and testing\n","train_dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True, collate_fn=pad_batch)\n","test_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=False, collate_fn=pad_batch)\n","\n","# Transformer Model\n","class TransformerRegressor(nn.Module):\n","    def __init__(self, input_dim=2, model_dim=64, num_heads=4, num_layers=2, ff_dim=128, dropout=0.1):\n","        super().__init__()\n","        self.embedding = nn.Linear(input_dim, model_dim)\n","        self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, \n","                                                        dim_feedforward=ff_dim, dropout=dropout, \n","                                                        batch_first=True)\n","        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n","        self.fc_out = nn.Linear(model_dim, 1)  # Output single angle value\n","\n","    def forward(self, x):\n","        x = self.embedding(x)  # (batch_size, seq_len, model_dim)\n","        x = self.transformer_encoder(x)  # (batch_size, seq_len, model_dim)\n","        x = x.mean(dim=1)  # Global average pooling\n","        return self.fc_out(x)  # (batch_size, 1)\n","\n","# Circular error function\n","def circular_error(pred, actual):\n","    \"\"\"\n","    Computes the circular error (in degrees) between predicted and actual angles.\n","    The error is the minimum of the absolute difference and 360 minus that difference.\n","    \"\"\"\n","    diff = abs(pred - actual)\n","    return diff if diff <= 180 else 360 - diff\n","\n","# Training Function\n","def train_model(model, train_dataloader, epochs=20, lr=0.001):\n","    model.to(device)  # Move the model to GPU (if available)\n","    criterion = nn.MSELoss().to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    \n","    for epoch in range(epochs):\n","        checkpoint_path = f\"checkpoint_angle_pred_keypoints_TBD_epoch_{epoch+1}.pth\"\n","        model.train()\n","        total_loss = 0\n","        for batch in train_dataloader:\n","            x, y = batch\n","            x, y = x, y  # Move data to GPU\n","            optimizer.zero_grad()\n","            output = model(x)\n","            loss = criterion(output, y)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","        if (epoch+1) % 40 == 0:\n","            torch.save(model.state_dict(), checkpoint_path)\n","        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_dataloader):.4f}\")\n","\n","# Testing Function\n","def test_model(model, test_dataloader):\n","    model.to(device)  # Ensure the model is on the correct device\n","    model.eval()\n","    total_loss = 0\n","    total_circular_error = 0\n","    criterion = nn.MSELoss()\n","    count = 0\n","    with torch.no_grad():\n","        for batch in tqdm(test_dataloader):\n","            x, y = batch\n","            x, y = x.to(device), y.to(device)  # Move data to GPU\n","            predictions = model(x)\n","            loss = criterion(predictions, y)\n","            total_loss += loss.item()\n","            \n","            # Compute and print circular error for each sample in the batch\n","            preds = predictions.squeeze()\n","            actuals = y.squeeze()\n","            # Ensure both preds and actuals are iterable\n","            if preds.dim() == 0:\n","                preds = preds.unsqueeze(0)\n","            if actuals.dim() == 0:\n","                actuals = actuals.unsqueeze(0)\n","            for pred, actual in zip(preds.tolist(), actuals.tolist()):\n","                err = circular_error(pred, actual)\n","                total_circular_error += err\n","                count += 1\n","                # print(f\"Actual: {actual:.2f}, Predicted: {pred:.2f}, Circular Error: {err:.2f}\")\n","    \n","    avg_loss = total_loss / len(test_dataloader)\n","    avg_circular_error = total_circular_error / count if count > 0 else 0\n","    print(f\"\\nTest Loss: {avg_loss:.4f}\")\n","    print(f\"Average Circular Error: {avg_circular_error:.2f}\")\n","\n","# Train the model\n","model = TransformerRegressor()\n","train_model(model, train_dataloader)\n","\n","# Test the model with detailed circular error analysis\n","test_model(model, test_dataloader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":7022607,"sourceId":11240975,"sourceType":"datasetVersion"}],"dockerImageVersionId":30919,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
