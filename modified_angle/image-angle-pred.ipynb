{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11240975,"sourceType":"datasetVersion","datasetId":7022607}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T16:07:03.175102Z","iopub.execute_input":"2025-04-01T16:07:03.175328Z","iopub.status.idle":"2025-04-01T16:07:04.119459Z","shell.execute_reply.started":"2025-04-01T16:07:03.175306Z","shell.execute_reply":"2025-04-01T16:07:04.118528Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"/kaggle/input/image-angle-pred-uhh-temp-yay-maybe/AngleOfPerson_20250330_175000.pkl\n/kaggle/input/image-angle-pred-uhh-temp-yay-maybe/AngleOfPerson_20250331_042254.pkl\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Running TBD Pedestrian, image only angle prediction","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport numpy as np\nfrom tqdm import tqdm\n\n###############################################################################\n# 1. CONFIGURATION\n###############################################################################\nDATA_PKL     = \"/kaggle/input/image-angle-pred-uhh-temp-yay-maybe/AngleOfPerson_20250331_042254.pkl\"  # <-- Replace with your path\nDEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE   = 100\nEPOCHS       = 20\nLEARNING_RATE = 0.004\nTRAIN_SPLIT  = 0.8\n# Basic image transforms\nIMAGE_TRANSFORM = transforms.Compose([\n    transforms.Resize((224, 224)), \n    transforms.ToTensor(),\n    # If using a pretrained ResNet, you typically want normalization:\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\nprint(f\"Using device: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T19:29:53.579756Z","iopub.execute_input":"2025-04-01T19:29:53.579999Z","iopub.status.idle":"2025-04-01T19:29:53.597356Z","shell.execute_reply.started":"2025-04-01T19:29:53.579970Z","shell.execute_reply":"2025-04-01T19:29:53.596657Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"###############################################################################\n# 2. DATASET\n###############################################################################\nclass ImageAngleDataset(Dataset):\n    \"\"\"\n    Dataset that returns:\n      - image tensor\n      - angle (float) as a label\n    \"\"\"\n    def __init__(self, data_list, transform=None):\n        \"\"\"\n        data_list: List of (image_path, angle)\n        transform: TorchVision transforms for images\n        \"\"\"\n        self.data_list = []\n        self.transform = transform\n        print(\"Creating dataset!\")\n        for item_a, item_b in tqdm(data_list):\n            modified_item_a = Image.fromarray(item_a)\n            if self.transform:\n                modified_item_a = self.transform(modified_item_a)#.to(DEVICE)\n            \n            modified_item_b = torch.tensor(item_b, dtype=torch.float32).unsqueeze(0)#.to(DEVICE)\n            self.data_list.append((modified_item_a, modified_item_b))\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def __getitem__(self, idx):\n        return self.data_list[idx]\n        # Load image\n        image = Image.fromarray(image_path)\n        # image = torch.from_numpy(np.transpose(image, (2,0,1))).float()\n        if self.transform:\n            image = self.transform(image)\n        # Convert angle to float tensor [1,]\n        angle_tensor = torch.tensor(angle, dtype=torch.float32).unsqueeze(0)\n        return image, angle_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T19:29:53.598132Z","iopub.execute_input":"2025-04-01T19:29:53.598434Z","iopub.status.idle":"2025-04-01T19:29:53.611052Z","shell.execute_reply.started":"2025-04-01T19:29:53.598405Z","shell.execute_reply":"2025-04-01T19:29:53.610356Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"###############################################################################\n# 3. MODEL: Simple ResNet-based regressor\n###############################################################################\nclass ImageRegressor(nn.Module):\n    def __init__(self, pretrained=True):\n        \"\"\"\n        If pretrained=True, uses pretrained ImageNet weights.\n        If pretrained=False, initializes from scratch.\n        \"\"\"\n        super().__init__()\n        # Use a ResNet18 as the backbone\n        if pretrained:\n            backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n        else:\n            backbone = models.resnet18(weights=None)\n\n        # Remove the final classification layer\n        num_feats = backbone.fc.in_features\n        backbone.fc = nn.Identity()\n\n        self.backbone = backbone\n        # Final linear to produce 1 output (angle)\n        #       But with an extra layer in between to smoothen the process\n        self.fc_before = nn.Linear(num_feats, 32)\n        self.fc = nn.Linear(32, 1)\n\n    def forward(self, x):\n        # x: [batch_size, 3, H, W]\n        features = self.backbone(x)   # [batch_size, 512] for ResNet18\n        before_out = self.fc_before(features)\n        out = self.fc(before_out)       # [batch_size, 1]\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T19:29:53.612746Z","iopub.execute_input":"2025-04-01T19:29:53.612959Z","iopub.status.idle":"2025-04-01T19:29:53.630773Z","shell.execute_reply.started":"2025-04-01T19:29:53.612940Z","shell.execute_reply":"2025-04-01T19:29:53.629937Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"###############################################################################\n# 4. TRAINING & TESTING\n###############################################################################\ndef circular_error(pred, actual):\n    \"\"\"\n    Computes the circular error (in degrees) between predicted and actual angles.\n    The error is defined as the minimum of the absolute difference and 360 minus that difference.\n    \"\"\"\n    diff = abs(pred - actual)\n    return diff if diff <= 180 else 360 - diff\n\ndef train_model(model, train_loader, epochs=10, lr=1e-3):\n    model.to(DEVICE)\n    criterion = nn.MSELoss().to(DEVICE)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n\n    for epoch in range(epochs):\n        checkpoint_path = f\"checkpoint_angle_pred_images_TBD_epoch_{epoch+1}.pth\"\n        model.train()\n        total_loss = 0.0\n\n        for images, angles in tqdm(train_loader):\n            images = images.to(DEVICE)\n            angles = angles.to(DEVICE)\n\n            optimizer.zero_grad()\n            preds = model(images)\n            loss = criterion(preds, angles)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch [{epoch+1}/{epochs}]  Train Loss: {avg_loss:.4f}\")\n        if (epoch+1) % 5 == 0:\n            torch.save(model.state_dict(), checkpoint_path)\n\ndef test_model(model, test_loader):\n    model.to(DEVICE)\n    model.eval()\n    criterion = nn.MSELoss()\n    total_loss = 0.0\n\n    # Optional: store predictions for further analysis\n    all_preds = []\n    all_labels = []\n    total_circular_error = 0.0\n    count = 0\n\n    with torch.no_grad():\n        for images, angles in tqdm(test_loader):\n            images = images.to(DEVICE)\n            angles = angles.to(DEVICE)\n\n            preds = model(images)\n            loss = criterion(preds, angles)\n            total_loss += loss.item()\n\n            preds_list = preds.cpu().view(-1).tolist()\n            angles_list = angles.cpu().view(-1).tolist()\n            all_preds.extend(preds_list)\n            all_labels.extend(angles_list)\n\n            for p, a in zip(preds_list, angles_list):\n                err = circular_error(p, a)\n                total_circular_error += err\n                count += 1\n\n    avg_loss = total_loss / len(test_loader)\n    avg_circular_error = total_circular_error / count if count > 0 else 0.0\n\n    print(f\"Test Loss: {avg_loss:.4f}\")\n    print(\"Sample Predictions vs Actual with Circular Error:\")\n    # for i in range(len(all_preds)):\n    #     err = circular_error(all_preds[i], all_labels[i])\n    #     print(f\"  Pred: {all_preds[i]:.2f}, Actual: {all_labels[i]:.2f}, Circular Error: {err:.2f}\")\n    print(f\"Average Circular Error: {avg_circular_error:.2f}\")\n    return all_preds, all_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T19:29:53.631717Z","iopub.execute_input":"2025-04-01T19:29:53.632087Z","iopub.status.idle":"2025-04-01T19:29:53.646417Z","shell.execute_reply.started":"2025-04-01T19:29:53.632024Z","shell.execute_reply":"2025-04-01T19:29:53.645602Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"# 1) Load data_list from .pkl\nwith open(DATA_PKL, \"rb\") as f:\n    data_list = pickle.load(f)\nprint(f\"Loaded {len(data_list)} samples from {DATA_PKL}\")\n\n# 2) Create dataset\ndataset = ImageAngleDataset(data_list, transform=IMAGE_TRANSFORM)\n\n# 3) Split into train/test\ntrain_size = int(TRAIN_SPLIT * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\ntest_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n\n# 4) Initialize model\nmodel = ImageRegressor(pretrained=True)\nprint(model)\n\n# 5) Train\nprint(\"Starting Training ...\")\ntrain_model(model, train_loader, epochs=EPOCHS, lr=LEARNING_RATE)\n\n# 6) Test\nprint(\"Starting Testing ...\")\ntest_model(model, test_loader)\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T19:29:53.647205Z","iopub.execute_input":"2025-04-01T19:29:53.647521Z","iopub.status.idle":"2025-04-01T19:49:27.076087Z","shell.execute_reply.started":"2025-04-01T19:29:53.647487Z","shell.execute_reply":"2025-04-01T19:49:27.075131Z"}},"outputs":[{"name":"stdout","text":"Loaded 35442 samples from /kaggle/input/image-angle-pred-uhh-temp-yay-maybe/AngleOfPerson_20250331_042254.pkl\nCreating dataset!\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 35442/35442 [00:56<00:00, 628.87it/s]\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 175MB/s]\n","output_type":"stream"},{"name":"stdout","text":"ImageRegressor(\n  (backbone): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): BasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): BasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): Identity()\n  )\n  (fc_before): Linear(in_features=512, out_features=32, bias=True)\n  (fc): Linear(in_features=32, out_features=1, bias=True)\n)\nStarting Training ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/20]  Train Loss: 4074.6598\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/20]  Train Loss: 2289.6262\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/20]  Train Loss: 1824.7184\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/20]  Train Loss: 1590.4435\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/20]  Train Loss: 1457.1083\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/20]  Train Loss: 1179.4808\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/20]  Train Loss: 929.6615\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/20]  Train Loss: 770.8657\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/20]  Train Loss: 645.2681\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/20]  Train Loss: 576.0873\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [11/20]  Train Loss: 556.2588\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [12/20]  Train Loss: 377.6495\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [13/20]  Train Loss: 295.6406\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [14/20]  Train Loss: 238.3053\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [15/20]  Train Loss: 269.3950\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [16/20]  Train Loss: 318.7275\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [17/20]  Train Loss: 344.0248\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [18/20]  Train Loss: 249.7903\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [19/20]  Train Loss: 143.3048\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 284/284 [00:55<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [20/20]  Train Loss: 100.9824\nStarting Testing ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 71/71 [00:06<00:00, 11.23it/s]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 878.7969\nSample Predictions vs Actual with Circular Error:\nAverage Circular Error: 12.95\nDone!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"checkpoint_path = \"/kaggle/working/checkpoint_angle_pred_images_TBD_epoch_20.pth\"\n\nif os.path.isfile(checkpoint_path):\n    print(\"Loading checkpoint...\")\n    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=True)\n    model.load_state_dict(checkpoint)\n\n    all_preds, all_labels = test_model(model, test_loader)\n    print(\"ACE result is\")\n    all_preds = np.array(all_preds) % 360\n    all_labels = np.array(all_labels) % 360\n    ACE = np.sum(abs(all_preds - all_labels)) / len(all_preds)\n    print(ACE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T19:49:32.021906Z","iopub.execute_input":"2025-04-01T19:49:32.022257Z","iopub.status.idle":"2025-04-01T19:49:38.456953Z","shell.execute_reply.started":"2025-04-01T19:49:32.022226Z","shell.execute_reply":"2025-04-01T19:49:38.455964Z"}},"outputs":[{"name":"stdout","text":"Loading checkpoint...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 71/71 [00:06<00:00, 11.17it/s]","output_type":"stream"},{"name":"stdout","text":"Test Loss: 878.7969\nSample Predictions vs Actual with Circular Error:\nAverage Circular Error: 12.95\nACE result is\n13.741358609401065\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":84},{"cell_type":"markdown","source":"# Creating keypoints annotations","metadata":{}},{"cell_type":"code","source":"import os\nfrom scipy.io import loadmat\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\nfrom tqdm import tqdm\nimport pickle\nimport datetime\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:44:26.965851Z","iopub.execute_input":"2025-04-01T17:44:26.966074Z","iopub.status.idle":"2025-04-01T17:44:32.179784Z","shell.execute_reply.started":"2025-04-01T17:44:26.966053Z","shell.execute_reply":"2025-04-01T17:44:32.178649Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"with open('/kaggle/input/image-angle-pred-uhh-temp-yay-maybe/AngleOfPerson_20250331_042254.pkl', 'rb') as f:\n    loaded_data = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:44:32.182328Z","iopub.execute_input":"2025-04-01T17:44:32.182871Z","iopub.status.idle":"2025-04-01T17:44:45.505844Z","shell.execute_reply.started":"2025-04-01T17:44:32.182835Z","shell.execute_reply":"2025-04-01T17:44:45.505114Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"### Stores the obtained images as a zip file in the given path\ndef obtain_all_pictures(full_data, path, name):\n    temp_path = os.path.join(path, 'temp_image/')\n    if not os.path.exists(temp_path):\n        os.makedirs(temp_path)\n\n    current_item = 0\n    for i in range(len(full_data)):\n        image = full_data[i][0]\n        image = Image.fromarray(image.astype('uint8')).convert('RGB')\n        image.save(os.path.join(temp_path, 'image_' + str(i) + '.jpg'))\n\n    # import shutil\n    # shutil.make_archive(os.path.join(path, name), 'zip', temp_path)\n    # shutil.rmtree(temp_path)\n\nobtain_all_pictures(loaded_data, \"/kaggle/working/annotation_data/images/\", 'images')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:44:45.507137Z","iopub.execute_input":"2025-04-01T17:44:45.507469Z","iopub.status.idle":"2025-04-01T17:44:57.037476Z","shell.execute_reply.started":"2025-04-01T17:44:45.507440Z","shell.execute_reply":"2025-04-01T17:44:57.036615Z"},"scrolled":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"with open('/kaggle/working/annotation_data/AngleOfPerson_20250331_042254.pkl', 'wb') as f:\n    pickle.dump(loaded_data, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:44:57.038303Z","iopub.execute_input":"2025-04-01T17:44:57.038530Z","iopub.status.idle":"2025-04-01T17:45:00.294481Z","shell.execute_reply.started":"2025-04-01T17:44:57.038512Z","shell.execute_reply":"2025-04-01T17:45:00.293544Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/annotation_data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T17:45:00.295485Z","iopub.execute_input":"2025-04-01T17:45:00.295825Z","iopub.status.idle":"2025-04-01T17:45:00.302416Z","shell.execute_reply.started":"2025-04-01T17:45:00.295793Z","shell.execute_reply":"2025-04-01T17:45:00.301609Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"['AngleOfPerson_20250331_042254.pkl', 'images']"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"!pip install ultralytics\nfrom ultralytics import YOLO\nimport os\nimport pickle\nfrom tqdm import tqdm\n\n# Load a COCO-pretrained YOLO12n model\nmodel = YOLO(\"yolo11m-pose.pt\")  # load an official model\n\ndef single_run(path_input, path_output):\n    results = model(path_input, save=True, show=True, show_conf=False, show_labels=False, max_det = 1, verbose=False)  # predict on an image\n\n    for result in results:\n        keypoints = result.keypoints  # Keypoints object for pose outputs\n\n    xyn = keypoints.xyn \n\n    f = open(path_output, \"w\")\n    for i in range(len(xyn)):\n        f.write(str(xyn[i]) + \"\\n\")\n    f.close()\n\n\npath_input = \"/kaggle/working/annotation_data/images/temp_image/\"\npath_output = \"/kaggle/working/annotation_data/KeypointsData.pkl\"\nimages_dir = path_input\noutput_pickle = path_output\nkeypoints_data = {}\nprint(len(os.listdir(images_dir)))\n# Process all images in the directory\nfor filename in tqdm(os.listdir(images_dir)):\n    if filename.endswith(\".jpg\"):\n        results = model(\n            os.path.join(images_dir, filename),\n            save=False,\n            show_conf=False,\n            show_labels=False,\n            verbose=False,\n            max_det=1,\n            device=DEVICE\n        )  # predict on an image\n\n        for result in results:\n            keypoints = result.keypoints  # Keypoints object for pose outputs\n            xyn = keypoints.xyn.tolist()  # Convert normalized coordinates to a list\n            keypoints_data[filename] = xyn  # Save keypoints for the image\nprint(len(keypoints_data))\n\n# Save the keypoints data to a pickle file\nwith open(output_pickle, \"wb\") as f:\n    pickle.dump(keypoints_data, f)\n\nprint(f\"Keypoints data saved to {output_pickle}\")\n\n# total_run(\"/kaggle/working/annotation_data/images/temp_image/\", \"kaggle/working/annotation_data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:05:07.627544Z","iopub.execute_input":"2025-04-01T18:05:07.627872Z","iopub.status.idle":"2025-04-01T18:15:06.147799Z","shell.execute_reply.started":"2025-04-01T18:05:07.627848Z","shell.execute_reply":"2025-04-01T18:15:06.146506Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.99)\nRequirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\n35442\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 35442/35442 [09:54<00:00, 59.58it/s]","output_type":"stream"},{"name":"stdout","text":"35442\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-9a0adeda7d32>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Save the keypoints data to a pickle file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeypoints_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kaggle/working/annotation_data/'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'kaggle/working/annotation_data/'","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"len(keypoints_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:19:02.963741Z","iopub.execute_input":"2025-04-01T18:19:02.964047Z","iopub.status.idle":"2025-04-01T18:19:02.968950Z","shell.execute_reply.started":"2025-04-01T18:19:02.964026Z","shell.execute_reply":"2025-04-01T18:19:02.968277Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"35442"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"os.listdir(\"/kaggle/working/annotation_data\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:17:00.560178Z","iopub.execute_input":"2025-04-01T18:17:00.560523Z","iopub.status.idle":"2025-04-01T18:17:00.566124Z","shell.execute_reply.started":"2025-04-01T18:17:00.560495Z","shell.execute_reply":"2025-04-01T18:17:00.565400Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"['KeypointsData.pkl', 'AngleOfPerson_20250331_042254.pkl', 'images']"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"len(os.listdir(\"/kaggle/working/annotation_data/images/temp_image\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:21:58.745488Z","iopub.execute_input":"2025-04-01T18:21:58.745775Z","iopub.status.idle":"2025-04-01T18:21:58.770119Z","shell.execute_reply.started":"2025-04-01T18:21:58.745753Z","shell.execute_reply":"2025-04-01T18:21:58.769450Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"35442"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"path_dir = \"/kaggle/working/annotation_data/\"\noutput_pickle = \"/kaggle/working/annotation_data/CombinedData.pkl\"\nimg_dir = \"/kaggle/working/annotation_data/images/temp_image/\"\ndirectory = path_dir\nresult = []\n\nwith open(directory + 'AngleOfPerson_20250331_042254.pkl', 'rb') as f:\n    loaded_data = pickle.load(f)\nwith open(directory + 'KeypointsData.pkl', 'rb') as f:\n    loaded_keypoints = pickle.load(f)\n\n# Iterate through the images (image_0.jpg to image_432.jpg)\nprint(len(loaded_keypoints))\nfor i in tqdm(range(len(loaded_keypoints))):\n    image_key = f\"image_{i}.jpg\"\n    if image_key in loaded_keypoints:\n        keypoints_value = loaded_keypoints[image_key]\n        angle_value = loaded_data[i][1]  # Get the second value from loaded_data[i]\n        result.append((keypoints_value, angle_value))  # Create the tuple and add to the list\n\n# output_pickle = \"C:/Users/Bulut/Documents/GitHub/Skelet/CombinedData.pkl\"\nwith open(output_pickle, \"wb\") as f:\n    pickle.dump(result, f)\n\nprint(f\"Keypoints data saved to {output_pickle}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:22:16.733061Z","iopub.execute_input":"2025-04-01T18:22:16.733358Z","iopub.status.idle":"2025-04-01T18:22:18.463595Z","shell.execute_reply.started":"2025-04-01T18:22:16.733337Z","shell.execute_reply":"2025-04-01T18:22:18.462634Z"}},"outputs":[{"name":"stdout","text":"35442\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 35442/35442 [00:00<00:00, 582869.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Keypoints data saved to /kaggle/working/annotation_data/CombinedData.pkl\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Adjust paths as needed\npkl_path = \"/kaggle/working/annotation_data/CombinedData.pkl\"\nimages_dir = \"/kaggle/working/annotation_data/images/temp_image/\"\noutput_pkl_path = \"/kaggle/working/annotation_data/FinalKeypointSet35K.pkl\"\n\n# Load your CombinedData.pkl\nwith open(pkl_path, 'rb') as f:\n    data_dict = pickle.load(f)  # Suppose it's a list of [keypoints, angle]\n    \n#print(f\"Loaded {len(data_dict)} samples from {pkl_path}\")\n#print(f\"Example data: {data_dict[0][0]}\")\n#print(f\"Example data: {data_dict[0][1]}\")\n\n\nfiltered_data_list = []\nprint(len(data_dict))\n\nfor i, item in enumerate(data_dict):\n    keypoints = item[0]\n    angle = torch.tensor(item[1], dtype=torch.float32).unsqueeze(0)   # Labels are the angle (single value)\n\n    # Convert to torch tensors for checking shape (or you can just check Python lists)\n    sequence = torch.tensor(keypoints, dtype=torch.float32)\n    if sequence.ndimension() == 3 and sequence.shape[0] == 1:\n        sequence = sequence.squeeze(0)  # Remove the first singleton dimension\n\n    # Check if sequence is empty or has invalid shape\n    # Example: we want sequence of shape (seq_len, 2) and seq_len>0\n    if sequence.shape[0] == 0 or sequence.shape[1] != 2:\n        # Skip this row\n        continue\n    \n    # If we reach here, the row is valid\n    # The corresponding image is: \"image_{i}.jpg\" or some pattern\n    image_path = os.path.join(images_dir, f\"image_{i}.jpg\")\n\n    # (Optionally) check if the image actually exists on disk\n    if not os.path.isfile(image_path):\n        print(f\"Warning: Image not found at {image_path}, skipping.\")\n        continue\n\n    # Keep the data\n    filtered_data_list.append((image_path, sequence, angle))\n\nprint(len(filtered_data_list))\n\n#If desired, save the filtered data to a new pkl\nwith open(output_pkl_path, 'wb') as f:\n    pickle.dump(filtered_data_list, f)\n\n\nprint(f\"Filtered data saved to: {output_pkl_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T18:54:56.977272Z","iopub.execute_input":"2025-04-01T18:54:56.977646Z","iopub.status.idle":"2025-04-01T18:55:03.296973Z","shell.execute_reply.started":"2025-04-01T18:54:56.977619Z","shell.execute_reply":"2025-04-01T18:55:03.296217Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"35442\n34657\nFiltered data saved to: /kaggle/working/annotation_data/FinalKeypointSet35K.pkl\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"# Running TBD Pedestrian, image + keypoints angle prediction","metadata":{}},{"cell_type":"code","source":"import pickle\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torch.nn.utils.rnn import pad_sequence\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Load dataset from .pkl file\ndirectory = \"/kaggle/working/annotation_data/\"  # Update this to the correct path\nwith open(directory + 'FinalKeypointSet35K.pkl', 'rb') as f:\n    data_dict = pickle.load(f)  # Assuming it's stored as a dictionary\n\n# Flatten data_dict to ensure each data sample is a 2D tensor of shape (seq_len, 2)\ndata = []\nlabels = []\nfor item in data_dict:\n    # Convert each sequence into a tensor of shape (seq_len, 2) and each label as a float tensor\n    sequence = torch.tensor(item[1], dtype=torch.float32)  # Ensure this is a tensor of shape (seq_len, 2)\n    label = torch.tensor(item[2], dtype=torch.float32).unsqueeze(0)   # Labels are the angle (single value)\n    data.append(sequence)\n    labels.append(label)\n\n# Padding function for variable-length sequences\ndef pad_batch(batch):\n    sequences, labels = zip(*batch)\n    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)  # Pad with zeros\n    return padded_sequences, torch.stack(labels)\n\n# Dataset Class\nclass CoordDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = [torch.tensor(seq, dtype=torch.float32).to(device) for seq in data]  # Convert to tensor\n        self.labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(device)  # Convert to tensor and reshape\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        x = self.data[idx]  # Tensor of shape (seq_len, 2)\n        y = self.labels[idx]  # Keep as a single value (not sin/cos)\n        return x, y\n\n# Split dataset into training and testing (90% training, 10% testing)\ndataset = CoordDataset(data, labels)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n\n# Create DataLoaders for training and testing\ntrain_dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True, collate_fn=pad_batch)\ntest_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=False, collate_fn=pad_batch)\n\n# Transformer Model\nclass TransformerRegressor(nn.Module):\n    def __init__(self, input_dim=2, model_dim=64, num_heads=4, num_layers=2, ff_dim=128, dropout=0.1):\n        super().__init__()\n        self.embedding = nn.Linear(input_dim, model_dim)\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, \n                                                        dim_feedforward=ff_dim, dropout=dropout, \n                                                        batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n        self.fc_out = nn.Linear(model_dim, 1)  # Output single angle value\n\n    def forward(self, x):\n        x = self.embedding(x)  # (batch_size, seq_len, model_dim)\n        x = self.transformer_encoder(x)  # (batch_size, seq_len, model_dim)\n        x = x.mean(dim=1)  # Global average pooling\n        return self.fc_out(x)  # (batch_size, 1)\n\n# Circular error function\ndef circular_error(pred, actual):\n    \"\"\"\n    Computes the circular error (in degrees) between predicted and actual angles.\n    The error is the minimum of the absolute difference and 360 minus that difference.\n    \"\"\"\n    diff = abs(pred - actual)\n    return diff if diff <= 180 else 360 - diff\n\n# Training Function\ndef train_model(model, train_dataloader, epochs=20, lr=0.001):\n    model.to(device)  # Move the model to GPU (if available)\n    criterion = nn.MSELoss().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        checkpoint_path = f\"checkpoint_angle_pred_keypoints_TBD_epoch_{epoch+1}.pth\"\n        model.train()\n        total_loss = 0\n        for batch in train_dataloader:\n            x, y = batch\n            x, y = x, y  # Move data to GPU\n            optimizer.zero_grad()\n            output = model(x)\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        if (epoch+1) % 40 == 0:\n            torch.save(model.state_dict(), checkpoint_path)\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_dataloader):.4f}\")\n\n# Testing Function\ndef test_model(model, test_dataloader):\n    model.to(device)  # Ensure the model is on the correct device\n    model.eval()\n    total_loss = 0\n    total_circular_error = 0\n    criterion = nn.MSELoss()\n    count = 0\n    with torch.no_grad():\n        for batch in tqdm(test_dataloader):\n            x, y = batch\n            x, y = x.to(device), y.to(device)  # Move data to GPU\n            predictions = model(x)\n            loss = criterion(predictions, y)\n            total_loss += loss.item()\n            \n            # Compute and print circular error for each sample in the batch\n            preds = predictions.squeeze()\n            actuals = y.squeeze()\n            # Ensure both preds and actuals are iterable\n            if preds.dim() == 0:\n                preds = preds.unsqueeze(0)\n            if actuals.dim() == 0:\n                actuals = actuals.unsqueeze(0)\n            for pred, actual in zip(preds.tolist(), actuals.tolist()):\n                err = circular_error(pred, actual)\n                total_circular_error += err\n                count += 1\n                # print(f\"Actual: {actual:.2f}, Predicted: {pred:.2f}, Circular Error: {err:.2f}\")\n    \n    avg_loss = total_loss / len(test_dataloader)\n    avg_circular_error = total_circular_error / count if count > 0 else 0\n    print(f\"\\nTest Loss: {avg_loss:.4f}\")\n    print(f\"Average Circular Error: {avg_circular_error:.2f}\")\n\n# Train the model\nmodel = TransformerRegressor()\ntrain_model(model, train_dataloader)\n\n# Test the model with detailed circular error analysis\ntest_model(model, test_dataloader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T19:28:30.023821Z","iopub.execute_input":"2025-04-01T19:28:30.024267Z","iopub.status.idle":"2025-04-01T19:29:53.505645Z","shell.execute_reply.started":"2025-04-01T19:28:30.024226Z","shell.execute_reply":"2025-04-01T19:29:53.504818Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-74-1fbbf4dc68de>:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  sequence = torch.tensor(item[1], dtype=torch.float32)  # Ensure this is a tensor of shape (seq_len, 2)\n<ipython-input-74-1fbbf4dc68de>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  label = torch.tensor(item[2], dtype=torch.float32).unsqueeze(0)   # Labels are the angle (single value)\n<ipython-input-74-1fbbf4dc68de>:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.data = [torch.tensor(seq, dtype=torch.float32).to(device) for seq in data]  # Convert to tensor\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20, Loss: 27926.6699\nEpoch 2/20, Loss: 16776.3227\nEpoch 3/20, Loss: 10011.9702\nEpoch 4/20, Loss: 8735.7185\nEpoch 5/20, Loss: 8109.4794\nEpoch 6/20, Loss: 7349.8417\nEpoch 7/20, Loss: 7202.8502\nEpoch 8/20, Loss: 7093.6007\nEpoch 9/20, Loss: 6958.2085\nEpoch 10/20, Loss: 6791.2280\nEpoch 11/20, Loss: 6664.6166\nEpoch 12/20, Loss: 6578.2953\nEpoch 13/20, Loss: 6457.5673\nEpoch 14/20, Loss: 6380.6527\nEpoch 15/20, Loss: 6350.3630\nEpoch 16/20, Loss: 6253.4224\nEpoch 17/20, Loss: 6216.8052\nEpoch 18/20, Loss: 6126.6757\nEpoch 19/20, Loss: 6114.6113\nEpoch 20/20, Loss: 6046.6032\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 70/70 [00:00<00:00, 268.84it/s]","output_type":"stream"},{"name":"stdout","text":"\nTest Loss: 5913.8797\nAverage Circular Error: 57.07\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}