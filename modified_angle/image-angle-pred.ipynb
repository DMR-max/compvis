{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-03T22:54:36.679813Z",
     "iopub.status.busy": "2025-04-03T22:54:36.679461Z",
     "iopub.status.idle": "2025-04-03T22:54:37.315115Z",
     "shell.execute_reply": "2025-04-03T22:54:37.313747Z",
     "shell.execute_reply.started": "2025-04-03T22:54:36.679783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running TBD Pedestrian, image only angle prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T23:39:11.301728Z",
     "iopub.status.busy": "2025-04-07T23:39:11.301447Z",
     "iopub.status.idle": "2025-04-07T23:39:11.308424Z",
     "shell.execute_reply": "2025-04-07T23:39:11.307506Z",
     "shell.execute_reply.started": "2025-04-07T23:39:11.301707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIGURATION\n",
    "###############################################################################\n",
    "DATA_PKL     = \"/kaggle/input/image-angle-pred-uhh-temp-yay-maybe/AngleOfPerson_20250331_042254.pkl\"  # <-- Replace with your path\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE   = 100\n",
    "EPOCHS       = 20\n",
    "LEARNING_RATE = 0.004\n",
    "TRAIN_SPLIT  = 0.8\n",
    "# Basic image transforms\n",
    "IMAGE_TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    # If using a pretrained ResNet, you typically want normalization:\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T23:39:11.428700Z",
     "iopub.status.busy": "2025-04-07T23:39:11.428414Z",
     "iopub.status.idle": "2025-04-07T23:39:11.434051Z",
     "shell.execute_reply": "2025-04-07T23:39:11.433148Z",
     "shell.execute_reply.started": "2025-04-07T23:39:11.428677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2. DATASET\n",
    "###############################################################################\n",
    "class ImageAngleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that returns:\n",
    "      - image tensor\n",
    "      - angle (float) as a label\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list, transform=None):\n",
    "        \"\"\"\n",
    "        data_list: List of (image_path, angle)\n",
    "        transform: TorchVision transforms for images\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.transform = transform\n",
    "        # print(\"Creating dataset!\")\n",
    "        # for item_a, item_b in tqdm(data_list):\n",
    "        #     modified_item_a = Image.fromarray(item_a)\n",
    "        #     if self.transform:\n",
    "        #         modified_item_a = self.transform(modified_item_a)#.to(DEVICE)\n",
    "            \n",
    "        #     modified_item_b = torch.tensor(item_b, dtype=torch.float32).unsqueeze(0)#.to(DEVICE)\n",
    "        #     self.data_list.append((modified_item_a, modified_item_b))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, angle = self.data_list[idx]\n",
    "        # Load image\n",
    "        image = Image.fromarray(image_path)\n",
    "        # image = torch.from_numpy(np.transpose(image, (2,0,1))).float()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Convert angle to float tensor [1,]\n",
    "        angle_tensor = torch.tensor(angle, dtype=torch.float32).unsqueeze(0)\n",
    "        return image, angle_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T23:39:11.569881Z",
     "iopub.status.busy": "2025-04-07T23:39:11.569603Z",
     "iopub.status.idle": "2025-04-07T23:39:11.575129Z",
     "shell.execute_reply": "2025-04-07T23:39:11.574178Z",
     "shell.execute_reply.started": "2025-04-07T23:39:11.569859Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 3. MODEL: Simple ResNet-based regressor\n",
    "###############################################################################\n",
    "class ImageRegressor(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        \"\"\"\n",
    "        If pretrained=True, uses pretrained ImageNet weights.\n",
    "        If pretrained=False, initializes from scratch.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Use a ResNet18 as the backbone\n",
    "        if pretrained:\n",
    "            backbone = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        else:\n",
    "            backbone = models.resnet18(weights=None)\n",
    "\n",
    "        # Remove the final classification layer\n",
    "        num_feats = backbone.fc.in_features\n",
    "        backbone.fc = nn.Identity()\n",
    "\n",
    "        self.backbone = backbone\n",
    "        # Final linear to produce 1 output (angle)\n",
    "        #       But with an extra layer in between to smoothen the process\n",
    "        self.fc_before = nn.Linear(num_feats, 32)\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, 3, H, W]\n",
    "        features = self.backbone(x)   # [batch_size, 512] for ResNet18\n",
    "        before_out = self.fc_before(features)\n",
    "        out = self.fc(before_out)       # [batch_size, 1]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T23:39:11.695596Z",
     "iopub.status.busy": "2025-04-07T23:39:11.695333Z",
     "iopub.status.idle": "2025-04-07T23:39:11.705383Z",
     "shell.execute_reply": "2025-04-07T23:39:11.704519Z",
     "shell.execute_reply.started": "2025-04-07T23:39:11.695572Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 4. TRAINING & TESTING\n",
    "###############################################################################\n",
    "def circular_error(pred, actual):\n",
    "    \"\"\"\n",
    "    Computes the circular error (in degrees) between predicted and actual angles.\n",
    "    The error is defined as the minimum of the absolute difference and 360 minus that difference.\n",
    "    \"\"\"\n",
    "    diff = abs(pred - actual)\n",
    "    return diff if diff <= 180 else 360 - diff\n",
    "\n",
    "def train_model(model, train_loader, epochs=10, lr=1e-3):\n",
    "    model.to(DEVICE)\n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        checkpoint_path = f\"checkpoint_angle_pred_images_TBD_epoch_{epoch+1}.pth\"\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for images, angles in tqdm(train_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            angles = angles.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(images)\n",
    "            \n",
    "            loss = criterion(preds, angles)\n",
    "            loss = torch.sum(torch.abs(180 - torch.abs(((torch.abs(angles - preds) % 360) - 180))))\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}]  Train Loss: {avg_loss:.4f}\")\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Optional: store predictions for further analysis\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_circular_error = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, angles in tqdm(test_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            angles = angles.to(DEVICE)\n",
    "\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, angles)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds_list = preds.cpu().view(-1).tolist()\n",
    "            angles_list = angles.cpu().view(-1).tolist()\n",
    "            all_preds.extend(preds_list)\n",
    "            all_labels.extend(angles_list)\n",
    "\n",
    "            for p, a in zip(preds_list, angles_list):\n",
    "                err = circular_error(p, a)\n",
    "                total_circular_error += err\n",
    "                count += 1\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    avg_circular_error = total_circular_error / count if count > 0 else 0.0\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(\"Sample Predictions vs Actual with Circular Error:\")\n",
    "    # for i in range(len(all_preds)):\n",
    "    #     err = circular_error(all_preds[i], all_labels[i])\n",
    "    #     print(f\"  Pred: {all_preds[i]:.2f}, Actual: {all_labels[i]:.2f}, Circular Error: {err:.2f}\")\n",
    "    print(f\"Average Circular Error: {avg_circular_error:.2f}\")\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T23:39:13.717872Z",
     "iopub.status.busy": "2025-04-07T23:39:13.717516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1) Load data_list from .pkl\n",
    "with open(DATA_PKL, \"rb\") as f:\n",
    "    data_list = pickle.load(f)\n",
    "print(f\"Loaded {len(data_list)} samples from {DATA_PKL}\")\n",
    "\n",
    "# 2) Create dataset\n",
    "dataset = ImageAngleDataset(data_list, transform=IMAGE_TRANSFORM)\n",
    "\n",
    "# 3) Split into train/test\n",
    "train_size = int(TRAIN_SPLIT * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# 4) Initialize model\n",
    "model = ImageRegressor(pretrained=True)\n",
    "print(model)\n",
    "\n",
    "# 5) Train\n",
    "print(\"Starting Training ...\")\n",
    "train_model(model, train_loader, epochs=EPOCHS, lr=LEARNING_RATE)\n",
    "\n",
    "# 6) Test\n",
    "print(\"Starting Testing ...\")\n",
    "test_model(model, test_loader)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/kaggle/working/checkpoint_angle_pred_images_TBD_epoch_20.pth\"\n",
    "\n",
    "if os.path.isfile(checkpoint_path):\n",
    "    print(\"Loading checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE, weights_only=True)\n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    all_preds, all_labels = test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating keypoints annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:36:18.735115Z",
     "iopub.status.busy": "2025-04-07T21:36:18.734748Z",
     "iopub.status.idle": "2025-04-07T21:36:23.396729Z",
     "shell.execute_reply": "2025-04-07T21:36:23.395357Z",
     "shell.execute_reply.started": "2025-04-07T21:36:18.735090Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy.io import loadmat\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import datetime\n",
    "from PIL import Image\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:36:23.398245Z",
     "iopub.status.busy": "2025-04-07T21:36:23.397870Z",
     "iopub.status.idle": "2025-04-07T21:36:37.548566Z",
     "shell.execute_reply": "2025-04-07T21:36:37.547390Z",
     "shell.execute_reply.started": "2025-04-07T21:36:23.398223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/image-angle-pred-uhh-temp-yay-maybe/AngleOfPerson_20250331_042254.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:36:37.551508Z",
     "iopub.status.busy": "2025-04-07T21:36:37.551124Z",
     "iopub.status.idle": "2025-04-07T21:36:49.120604Z",
     "shell.execute_reply": "2025-04-07T21:36:49.119779Z",
     "shell.execute_reply.started": "2025-04-07T21:36:37.551475Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### Stores the obtained images as a zip file in the given path\n",
    "def obtain_all_pictures(full_data, path, name):\n",
    "    temp_path = os.path.join(path, 'temp_image/')\n",
    "    if not os.path.exists(temp_path):\n",
    "        os.makedirs(temp_path)\n",
    "\n",
    "    current_item = 0\n",
    "    for i in range(len(full_data)):\n",
    "        image = full_data[i][0]\n",
    "        image = Image.fromarray(image.astype('uint8')).convert('RGB')\n",
    "        image.save(os.path.join(temp_path, 'image_' + str(i) + '.jpg'))\n",
    "\n",
    "    # import shutil\n",
    "    # shutil.make_archive(os.path.join(path, name), 'zip', temp_path)\n",
    "    # shutil.rmtree(temp_path)\n",
    "\n",
    "obtain_all_pictures(loaded_data, \"/kaggle/working/annotation_data/images/\", 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:36:49.121892Z",
     "iopub.status.busy": "2025-04-07T21:36:49.121654Z",
     "iopub.status.idle": "2025-04-07T21:36:59.315869Z",
     "shell.execute_reply": "2025-04-07T21:36:59.314858Z",
     "shell.execute_reply.started": "2025-04-07T21:36:49.121873Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/working/annotation_data/AngleOfPerson_20250331_042254.pkl', 'wb') as f:\n",
    "    pickle.dump(loaded_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:36:59.317401Z",
     "iopub.status.busy": "2025-04-07T21:36:59.317039Z",
     "iopub.status.idle": "2025-04-07T21:36:59.324245Z",
     "shell.execute_reply": "2025-04-07T21:36:59.323166Z",
     "shell.execute_reply.started": "2025-04-07T21:36:59.317369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.listdir(\"/kaggle/working/annotation_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-07T21:36:59.325743Z",
     "iopub.status.busy": "2025-04-07T21:36:59.325444Z",
     "iopub.status.idle": "2025-04-07T21:37:02.962470Z",
     "shell.execute_reply": "2025-04-07T21:37:02.961337Z",
     "shell.execute_reply.started": "2025-04-07T21:36:59.325713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loaded_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "execution_failed": "2025-04-07T22:05:07.351Z",
     "iopub.execute_input": "2025-04-07T21:37:02.963624Z",
     "iopub.status.busy": "2025-04-07T21:37:02.963337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load a COCO-pretrained YOLO12n model\n",
    "model = YOLO(\"yolo11m-pose.pt\")  # load an official model\n",
    "\n",
    "def single_run(path_input, path_output):\n",
    "    results = model(path_input, save=True, show=True, show_conf=False, show_labels=False, max_det = 1, verbose=False)  # predict on an image\n",
    "\n",
    "    for result in results:\n",
    "        keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "\n",
    "    xyn = keypoints.xyn \n",
    "\n",
    "    f = open(path_output, \"w\")\n",
    "    for i in range(len(xyn)):\n",
    "        f.write(str(xyn[i]) + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "path_input = \"/kaggle/working/annotation_data/images/temp_image/\"\n",
    "path_output = \"/kaggle/working/annotation_data/KeypointsData.pkl\"\n",
    "images_dir = path_input\n",
    "output_pickle = path_output\n",
    "keypoints_data = {}\n",
    "print(len(os.listdir(images_dir)))\n",
    "# Process all images in the directory\n",
    "for filename in tqdm(os.listdir(images_dir)):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        results = model(\n",
    "            os.path.join(images_dir, filename),\n",
    "            save=False,\n",
    "            show_conf=False,\n",
    "            show_labels=False,\n",
    "            verbose=False,\n",
    "            max_det=1,\n",
    "            device=DEVICE\n",
    "        )  # predict on an image\n",
    "\n",
    "        for result in results:\n",
    "            keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "            xyn = keypoints.xyn.tolist()  # Convert normalized coordinates to a list\n",
    "            keypoints_data[filename] = xyn  # Save keypoints for the image\n",
    "print(len(keypoints_data))\n",
    "\n",
    "# Save the keypoints data to a pickle file\n",
    "with open(output_pickle, \"wb\") as f:\n",
    "    pickle.dump(keypoints_data, f)\n",
    "\n",
    "print(f\"Keypoints data saved to {output_pickle}\")\n",
    "\n",
    "# total_run(\"/kaggle/working/annotation_data/images/temp_image/\", \"kaggle/working/annotation_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T22:05:07.352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(keypoints_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T22:05:07.352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.listdir(\"/kaggle/working/annotation_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T22:05:07.352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(os.listdir(\"/kaggle/working/annotation_data/images/temp_image\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T22:05:07.352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "path_dir = \"/kaggle/working/annotation_data/\"\n",
    "output_pickle = \"/kaggle/working/annotation_data/CombinedData.pkl\"\n",
    "img_dir = \"/kaggle/working/annotation_data/images/temp_image/\"\n",
    "directory = path_dir\n",
    "result = []\n",
    "\n",
    "with open(directory + 'AngleOfPerson_20250331_042254.pkl', 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "with open(directory + 'KeypointsData.pkl', 'rb') as f:\n",
    "    loaded_keypoints = pickle.load(f)\n",
    "\n",
    "# Iterate through the images (image_0.jpg to image_432.jpg)\n",
    "print(len(loaded_keypoints))\n",
    "for i in tqdm(range(len(loaded_keypoints))):\n",
    "    image_key = f\"image_{i}.jpg\"\n",
    "    if image_key in loaded_keypoints:\n",
    "        keypoints_value = loaded_keypoints[image_key]\n",
    "        angle_value = loaded_data[i][1]  # Get the second value from loaded_data[i]\n",
    "        result.append((keypoints_value, angle_value))  # Create the tuple and add to the list\n",
    "\n",
    "# output_pickle = \"C:/Users/Bulut/Documents/GitHub/Skelet/CombinedData.pkl\"\n",
    "with open(output_pickle, \"wb\") as f:\n",
    "    pickle.dump(result, f)\n",
    "\n",
    "print(f\"Keypoints data saved to {output_pickle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-07T22:05:07.352Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Adjust paths as needed\n",
    "pkl_path = \"/kaggle/working/annotation_data/CombinedData.pkl\"\n",
    "images_dir = \"/kaggle/working/annotation_data/images/temp_image/\"\n",
    "output_pkl_path = \"/kaggle/working/annotation_data/FinalKeypointSet35K.pkl\"\n",
    "\n",
    "# Load your CombinedData.pkl\n",
    "with open(pkl_path, 'rb') as f:\n",
    "    data_dict = pickle.load(f)  # Suppose it's a list of [keypoints, angle]\n",
    "    \n",
    "#print(f\"Loaded {len(data_dict)} samples from {pkl_path}\")\n",
    "#print(f\"Example data: {data_dict[0][0]}\")\n",
    "#print(f\"Example data: {data_dict[0][1]}\")\n",
    "\n",
    "\n",
    "filtered_data_list = []\n",
    "print(len(data_dict))\n",
    "\n",
    "for i, item in enumerate(data_dict):\n",
    "    keypoints = item[0]\n",
    "    angle = torch.tensor(item[1], dtype=torch.float32).unsqueeze(0)   # Labels are the angle (single value)\n",
    "\n",
    "    # Convert to torch tensors for checking shape (or you can just check Python lists)\n",
    "    sequence = torch.tensor(keypoints, dtype=torch.float32)\n",
    "    if sequence.ndimension() == 3 and sequence.shape[0] == 1:\n",
    "        sequence = sequence.squeeze(0)  # Remove the first singleton dimension\n",
    "\n",
    "    # Check if sequence is empty or has invalid shape\n",
    "    # Example: we want sequence of shape (seq_len, 2) and seq_len>0\n",
    "    if sequence.shape[0] == 0 or sequence.shape[1] != 2:\n",
    "        # Skip this row\n",
    "        continue\n",
    "    \n",
    "    # If we reach here, the row is valid\n",
    "    # The corresponding image is: \"image_{i}.jpg\" or some pattern\n",
    "    image_path = os.path.join(images_dir, f\"image_{i}.jpg\")\n",
    "\n",
    "    # (Optionally) check if the image actually exists on disk\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(f\"Warning: Image not found at {image_path}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Keep the data\n",
    "    filtered_data_list.append((image_path, sequence, angle))\n",
    "\n",
    "print(len(filtered_data_list))\n",
    "\n",
    "#If desired, save the filtered data to a new pkl\n",
    "with open(output_pkl_path, 'wb') as f:\n",
    "    pickle.dump(filtered_data_list, f)\n",
    "\n",
    "\n",
    "print(f\"Filtered data saved to: {output_pkl_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running TBD Pedestrian, keypoints angle prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T01:45:10.077379Z",
     "iopub.status.busy": "2025-04-04T01:45:10.077072Z",
     "iopub.status.idle": "2025-04-04T01:45:54.565593Z",
     "shell.execute_reply": "2025-04-04T01:45:54.564274Z",
     "shell.execute_reply.started": "2025-04-04T01:45:10.077357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset from .pkl file\n",
    "directory = \"/kaggle/working/annotation_data/\"  # Update this to the correct path\n",
    "with open(directory + 'FinalKeypointSet35K.pkl', 'rb') as f:\n",
    "    data_dict = pickle.load(f)  # Assuming it's stored as a dictionary\n",
    "\n",
    "# Flatten data_dict to ensure each data sample is a 2D tensor of shape (seq_len, 2)\n",
    "data = []\n",
    "labels = []\n",
    "for item in data_dict:\n",
    "    # Convert each sequence into a tensor of shape (seq_len, 2) and each label as a float tensor\n",
    "    sequence = torch.tensor(item[1], dtype=torch.float32)  # Ensure this is a tensor of shape (seq_len, 2)\n",
    "    label = torch.tensor(item[2], dtype=torch.float32).unsqueeze(0)   # Labels are the angle (single value)\n",
    "    data.append(sequence)\n",
    "    labels.append(label)\n",
    "\n",
    "# Padding function for variable-length sequences\n",
    "def pad_batch(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True, padding_value=0.0)  # Pad with zeros\n",
    "    return padded_sequences, torch.stack(labels)\n",
    "\n",
    "# Dataset Class\n",
    "class CoordDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = [torch.tensor(seq, dtype=torch.float32).to(device) for seq in data]  # Convert to tensor\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32).unsqueeze(1).to(device)  # Convert to tensor and reshape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]  # Tensor of shape (seq_len, 2)\n",
    "        y = self.labels[idx]  # Keep as a single value (not sin/cos)\n",
    "        return x, y\n",
    "\n",
    "# Split dataset into training and testing (90% training, 10% testing)\n",
    "dataset = CoordDataset(data, labels)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoaders for training and testing\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True, collate_fn=pad_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=False, collate_fn=pad_batch)\n",
    "\n",
    "# Transformer Model\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim=2, model_dim=64, num_heads=4, num_layers=2, ff_dim=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, \n",
    "                                                        dim_feedforward=ff_dim, dropout=dropout, \n",
    "                                                        batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(model_dim, 1)  # Output single angle value\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, model_dim)\n",
    "        x = self.transformer_encoder(x)  # (batch_size, seq_len, model_dim)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        return self.fc_out(x)  # (batch_size, 1)\n",
    "\n",
    "# Circular error function\n",
    "def circular_error(pred, actual):\n",
    "    \"\"\"\n",
    "    Computes the circular error (in degrees) between predicted and actual angles.\n",
    "    The error is the minimum of the absolute difference and 360 minus that difference.\n",
    "    \"\"\"\n",
    "    diff = abs(pred - actual)\n",
    "    return diff if diff <= 180 else 360 - diff\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_dataloader, epochs=200, lr=0.0005):\n",
    "    model.to(device)  # Move the model to GPU (if available)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        checkpoint_path = f\"checkpoint_angle_pred_keypoints_TBD_epoch_{epoch+1}.pth\"\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            x, y = batch\n",
    "            x, y = x, y  # Move data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if (epoch+1) % 40 == 0:\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "# Testing Function\n",
    "def test_model(model, test_dataloader):\n",
    "    model.to(device)  # Ensure the model is on the correct device\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_circular_error = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)  # Move data to GPU\n",
    "            predictions = model(x)\n",
    "            loss = criterion(predictions, y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Compute and print circular error for each sample in the batch\n",
    "            preds = predictions.squeeze()\n",
    "            actuals = y.squeeze()\n",
    "            # Ensure both preds and actuals are iterable\n",
    "            if preds.dim() == 0:\n",
    "                preds = preds.unsqueeze(0)\n",
    "            if actuals.dim() == 0:\n",
    "                actuals = actuals.unsqueeze(0)\n",
    "            for pred, actual in zip(preds.tolist(), actuals.tolist()):\n",
    "                err = circular_error(pred, actual)\n",
    "                total_circular_error += err\n",
    "                count += 1\n",
    "                # print(f\"Actual: {actual:.2f}, Predicted: {pred:.2f}, Circular Error: {err:.2f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(test_dataloader)\n",
    "    avg_circular_error = total_circular_error / count if count > 0 else 0\n",
    "    print(f\"\\nTest Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Average Circular Error: {avg_circular_error:.2f}\")\n",
    "\n",
    "# Train the model\n",
    "model = TransformerRegressor()\n",
    "train_model(model, train_dataloader)\n",
    "\n",
    "# Test the model with detailed circular error analysis\n",
    "test_model(model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T01:08:00.319983Z",
     "iopub.status.busy": "2025-04-04T01:08:00.319568Z",
     "iopub.status.idle": "2025-04-04T01:44:39.979678Z",
     "shell.execute_reply": "2025-04-04T01:44:39.978487Z",
     "shell.execute_reply.started": "2025-04-04T01:08:00.319951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "###############################################################################\n",
    "# 1. CONFIGURATION\n",
    "###############################################################################\n",
    "FILTERED_DATA_PKL = \"/kaggle/working/annotation_data/FinalKeypointSet35K.pkl\"\n",
    "DEVICE            = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Example transforms for images (resize to 224x224, convert to tensor)\n",
    "IMAGE_TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # If using pretrained networks, you often do:\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "BATCH_SIZE    = 100\n",
    "TRAIN_SPLIT   = 0.8\n",
    "EPOCHS        = 20\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "###############################################################################\n",
    "# 2. DATASET & DATALOADER\n",
    "###############################################################################\n",
    "\n",
    "class ImageKeypointDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset that provides:\n",
    "      1) Image,\n",
    "      2) Raw keypoint coordinates,\n",
    "      3) Label (angle of movement).\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list, transform=None):\n",
    "        \"\"\"\n",
    "        data_list: a list of tuples -> (image_path, keypoints, angle)\n",
    "        transform: torchvision transforms for images\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.transform = transform\n",
    "        # print(\"Initializing dataset!\")\n",
    "        # for i in tqdm(range(len(data_list))):\n",
    "        #     image_path, keypoints, angle = data_list[i]\n",
    "        #     image = Image.open(image_path).convert(\"RGB\")\n",
    "        #     if self.transform:\n",
    "        #         image = self.transform(image)\n",
    "\n",
    "        #     # Convert keypoints to tensor (seq_len, 2)\n",
    "        #     kp_tensor = torch.tensor(keypoints, dtype=torch.float32)\n",
    "    \n",
    "        #     # Convert angle to float tensor, shape (1,)\n",
    "        #     angle_tensor = torch.tensor(angle, dtype=torch.float32).unsqueeze(0)\n",
    "        #     self.data_list.append((image, kp_tensor, angle_tensor))\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return self.data_list[idx]\n",
    "        image_path, keypoints, angle = self.data_list[idx]\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert keypoints to tensor (seq_len, 2)\n",
    "        kp_tensor = torch.tensor(keypoints, dtype=torch.float32)\n",
    "\n",
    "        # Convert angle to float tensor, shape (1,)\n",
    "        angle_tensor = angle#torch.tensor(angle, dtype=torch.float32)#.unsqueeze(0)\n",
    "\n",
    "        return image, kp_tensor, angle_tensor\n",
    "\n",
    "\n",
    "def multimodal_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle:\n",
    "      - a list of (image, keypoints, label)\n",
    "      - variable-length keypoints\n",
    "      - images get stacked\n",
    "    \"\"\"\n",
    "    images, kpoints_list, labels = zip(*batch)\n",
    "\n",
    "    # Stack images\n",
    "    images = torch.stack(images, dim=0)  # (batch_size, C, H, W)\n",
    "\n",
    "    # Pad keypoints to the same sequence length\n",
    "    padded_kpoints = pad_sequence(kpoints_list, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    # Stack labels\n",
    "    labels = torch.stack(labels, dim=0)  # (batch_size, 1)\n",
    "\n",
    "    return images, padded_kpoints, labels\n",
    "\n",
    "###############################################################################\n",
    "# 3. MULTI-MODAL MODEL\n",
    "###############################################################################\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, pretrained=True, out_features=128):\n",
    "        super().__init__()\n",
    "        # Use a pretrained ResNet18 as an example\n",
    "        backbone = models.resnet18(pretrained=pretrained)\n",
    "        # Remove final classification layer\n",
    "        num_feats = backbone.fc.in_features\n",
    "        backbone.fc = nn.Identity()\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        self.projection = nn.Linear(num_feats, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, 3, H, W)\n",
    "        \"\"\"\n",
    "        features = self.backbone(x)         # (batch_size, 512) for ResNet18\n",
    "        out = self.projection(features)     # (batch_size, out_features)\n",
    "        return out\n",
    "\n",
    "\n",
    "class KeypointEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=2, model_dim=64, num_heads=4, num_layers=2, ff_dim=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        \n",
    "        # Note: remove batch_first here\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=ff_dim,\n",
    "            dropout=dropout\n",
    "            # batch_first=False is default\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(model_dim, model_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: [batch_size, seq_len, 2]\n",
    "        \"\"\"\n",
    "        # 1) Embed => [batch_size, seq_len, model_dim]\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # 2) Transpose => [seq_len, batch_size, model_dim]\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # 3) Pass through transformer => still [seq_len, batch_size, model_dim]\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # 4) Transpose back => [batch_size, seq_len, model_dim]\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # 5) Pool across seq_len => [batch_size, model_dim]\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        # 6) Final FC => [batch_size, model_dim]\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiModalRegressor(nn.Module):\n",
    "    def __init__(self, img_out_features=128, keypoint_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.img_encoder = ImageEncoder(pretrained=True, out_features=img_out_features)\n",
    "        self.kp_encoder  = KeypointEncoder(input_dim=2, model_dim=keypoint_dim)\n",
    "\n",
    "        fusion_input_dim = img_out_features + keypoint_dim\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # single angle\n",
    "        )\n",
    "\n",
    "    def forward(self, images, keypoints):\n",
    "        # Encode image\n",
    "        img_feats = self.img_encoder(images)   # (batch_size, img_out_features)\n",
    "\n",
    "        # Encode keypoints\n",
    "        kp_feats  = self.kp_encoder(keypoints) # (batch_size, keypoint_dim)\n",
    "        # print(img_feats)\n",
    "        # print(kp_feats)\n",
    "        # Fuse\n",
    "        fused = torch.cat([img_feats, kp_feats], dim=1)  # (batch_size, fusion_input_dim)\n",
    "        # print(fused)\n",
    "\n",
    "        # Regress angle\n",
    "        out = self.regressor(fused)  # (batch_size, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4. TRAINING & TESTING FUNCTIONS\n",
    "###############################################################################\n",
    "\n",
    "def train_model(model, train_loader, epochs=10, lr=1e-3):\n",
    "    model.to(DEVICE)\n",
    "    model.train()\n",
    "    criterion = nn.MSELoss().to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for images, kpoints, labels in tqdm(train_loader):\n",
    "            checkpoint_path = f\"checkpoint_angle_pred_keypoints_image_TBD_epoch_{epoch+1}.pth\"\n",
    "            images = images.to(DEVICE)\n",
    "            kpoints = kpoints.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(images, kpoints)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            # print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_dataloader):.4f}\")\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, kpoints, labels in test_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            kpoints = kpoints.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            preds = model(images, kpoints)\n",
    "            loss = criterion(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # If you want to visualize some predictions:\n",
    "            # print(\"Predicted:\", preds.squeeze().tolist())\n",
    "            # print(\"Actual:   \", labels.squeeze().tolist())\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5. MAIN EXECUTION\n",
    "###############################################################################\n",
    "\n",
    "# 1) Load filtered data\n",
    "with open(FILTERED_DATA_PKL, 'rb') as f:\n",
    "    filtered_data_list = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded filtered data list, total samples: {len(filtered_data_list)}\")\n",
    "\n",
    "# 2) Create dataset\n",
    "dataset = ImageKeypointDataset(filtered_data_list, transform=IMAGE_TRANSFORM)\n",
    "\n",
    "# 3) Train/Test split\n",
    "train_size = int(TRAIN_SPLIT * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 4) Dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=multimodal_collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=multimodal_collate_fn\n",
    ")\n",
    "\n",
    "# 5) Initialize multi-modal model\n",
    "model = MultiModalRegressor(\n",
    "    img_out_features=128,\n",
    "    keypoint_dim=64,\n",
    "    hidden_dim=32\n",
    ")\n",
    "print(model)\n",
    "\n",
    "# 6) Train\n",
    "print(\"Starting Training ...\")\n",
    "train_model(model, train_loader, epochs=EPOCHS, lr=LEARNING_RATE)\n",
    "\n",
    "# 7) Test\n",
    "print(\"Starting Testing ...\")\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-04T00:14:39.776179Z",
     "iopub.status.busy": "2025-04-04T00:14:39.775889Z",
     "iopub.status.idle": "2025-04-04T00:14:58.140650Z",
     "shell.execute_reply": "2025-04-04T00:14:58.139754Z",
     "shell.execute_reply.started": "2025-04-04T00:14:39.776158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Circular error function\n",
    "def circular_error(pred, actual):\n",
    "    \"\"\"\n",
    "    Computes the circular error (in degrees) between predicted and actual angles.\n",
    "    The error is the minimum of the absolute difference and 360 minus that difference.\n",
    "    \"\"\"\n",
    "    diff = abs(pred - actual)\n",
    "    return diff if diff <= 180 else 360 - diff\n",
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    criterion = nn.MSELoss()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Optional: store predictions for further analysis\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_circular_error = 0.0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, kpoints, labels in tqdm(test_loader):\n",
    "            images = images.to(DEVICE)\n",
    "            kpoints = kpoints.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            preds = model(images, kpoints)\n",
    "            loss = criterion(preds, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds_list = preds.cpu().view(-1).tolist()\n",
    "            angles_list = labels.cpu().view(-1).tolist()\n",
    "            all_preds.extend(preds_list)\n",
    "            all_labels.extend(angles_list)\n",
    "            \n",
    "            for p, a in zip(preds_list, angles_list):\n",
    "                err = circular_error(p, a)\n",
    "                total_circular_error += err\n",
    "                count += 1\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    ace = total_circular_error / count\n",
    "    print(f\"ACE: {ace:.4f}\")\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T22:54:13.109313Z",
     "iopub.status.busy": "2025-04-03T22:54:13.109035Z",
     "iopub.status.idle": "2025-04-03T22:54:13.112750Z",
     "shell.execute_reply": "2025-04-03T22:54:13.111909Z",
     "shell.execute_reply.started": "2025-04-03T22:54:13.109292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7022607,
     "sourceId": 11240975,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
